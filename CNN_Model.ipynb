{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout,BatchNormalization\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35882</th>\n",
       "      <td>6</td>\n",
       "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35883</th>\n",
       "      <td>3</td>\n",
       "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>0</td>\n",
       "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>3</td>\n",
       "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>2</td>\n",
       "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion                                             pixels        Usage\n",
       "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
       "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
       "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
       "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
       "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert pixel strings to numpy arrays\n",
    "data['pixels'] = data['pixels'].apply(lambda x: np.array(x.split(), dtype='float32'))\n",
    "\n",
    "# Normalize the pixel values\n",
    "data['pixels'] = data['pixels']/ 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.27450982, 0.3137255, 0.32156864, 0.28235295...\n",
       "1        [0.5921569, 0.5882353, 0.5764706, 0.60784316, ...\n",
       "2        [0.90588236, 0.83137256, 0.6117647, 0.6431373,...\n",
       "3        [0.09411765, 0.1254902, 0.14117648, 0.11764706...\n",
       "4        [0.015686275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....\n",
       "                               ...                        \n",
       "35882    [0.19607843, 0.14117648, 0.06666667, 0.0862745...\n",
       "35883    [0.69803923, 0.68235296, 0.6745098, 0.6784314,...\n",
       "35884    [0.06666667, 0.06666667, 0.0627451, 0.09019608...\n",
       "35885    [0.11764706, 0.10980392, 0.10980392, 0.1137254...\n",
       "35886    [0.07450981, 0.050980393, 0.05490196, 0.047058...\n",
       "Name: pixels, Length: 35887, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pixels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_data = data[data['Usage'] == 'Training']\n",
    "val_data = data[data['Usage'] == 'PublicTest']\n",
    "test_data = data[data['Usage'] == 'PrivateTest']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pixel values and emotion labels\n",
    "\n",
    "X_train = np.stack(train_data['pixels'].values)\n",
    "y_train = train_data['emotion'].values\n",
    "\n",
    "X_val = np.stack(val_data['pixels'].values)\n",
    "y_val = val_data['emotion'].values\n",
    "\n",
    "X_test = np.stack(test_data['pixels'].values)\n",
    "y_test = test_data['emotion'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting the dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the images or the features so the model can use it\n",
    "X_train = X_train.reshape((X_train.shape[0], 48 , 48, 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], 48 , 48, 1 ))\n",
    "X_test = X_test.reshape((X_test.shape[0],  48 , 48, 1 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode emotion labels\n",
    "num_classes = 7  #7 emotion classes\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_val = to_categorical(y_val, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_56 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 12, 12, 512)       590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 4,496,903\n",
      "Trainable params: 4,492,935\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hazem\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model= tf.keras.models.Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n",
    "model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(lr=0.0001), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    "  )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "449/449 [==============================] - 16s 33ms/step - loss: 9.1259 - accuracy: 0.2141 - val_loss: 8.4447 - val_accuracy: 0.2600\n",
      "Epoch 2/16\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 7.8237 - accuracy: 0.2745 - val_loss: 6.9718 - val_accuracy: 0.3491\n",
      "Epoch 3/16\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 6.5860 - accuracy: 0.3311 - val_loss: 5.8696 - val_accuracy: 0.3865\n",
      "Epoch 4/16\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 5.5126 - accuracy: 0.3695 - val_loss: 5.0444 - val_accuracy: 0.3823\n",
      "Epoch 5/16\n",
      "449/449 [==============================] - 15s 32ms/step - loss: 4.6070 - accuracy: 0.4083 - val_loss: 4.1033 - val_accuracy: 0.4539\n",
      "Epoch 6/16\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 3.8793 - accuracy: 0.4421 - val_loss: 3.4450 - val_accuracy: 0.4929\n",
      "Epoch 7/16\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 3.2997 - accuracy: 0.4769 - val_loss: 3.0223 - val_accuracy: 0.5160\n",
      "Epoch 8/16\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 2.8563 - accuracy: 0.5041 - val_loss: 2.6103 - val_accuracy: 0.5355\n",
      "Epoch 9/16\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 2.5038 - accuracy: 0.5232 - val_loss: 2.3249 - val_accuracy: 0.5522\n",
      "Epoch 10/16\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 2.2398 - accuracy: 0.5459 - val_loss: 2.1168 - val_accuracy: 0.5614\n",
      "Epoch 11/16\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 2.0368 - accuracy: 0.5654 - val_loss: 1.9203 - val_accuracy: 0.5860\n",
      "Epoch 12/16\n",
      "449/449 [==============================] - 15s 32ms/step - loss: 1.8924 - accuracy: 0.5760 - val_loss: 1.8677 - val_accuracy: 0.5762\n",
      "Epoch 13/16\n",
      "449/449 [==============================] - 15s 32ms/step - loss: 1.7742 - accuracy: 0.5979 - val_loss: 1.7660 - val_accuracy: 0.5954\n",
      "Epoch 14/16\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 1.6859 - accuracy: 0.6097 - val_loss: 1.7666 - val_accuracy: 0.5823\n",
      "Epoch 15/16\n",
      "449/449 [==============================] - 15s 32ms/step - loss: 1.6196 - accuracy: 0.6279 - val_loss: 1.7274 - val_accuracy: 0.6032\n",
      "Epoch 16/16\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 1.5547 - accuracy: 0.6479 - val_loss: 1.6967 - val_accuracy: 0.6102\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=16, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_pixels function used to extract features from the new images\n",
    "def preprocess_pixels(pixels):\n",
    "    pixels = pixels.astype('float32')\n",
    "    pixels = pixels.reshape((1, 48, 48, 1))\n",
    "    pixels = pixels / 255.0\n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def classify_image(image_path):\n",
    "    # Read the image file\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize the image to the desired dimensions\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    \n",
    "    # Convert the image into pixel values\n",
    "    pixels = np.array(image)\n",
    "    \n",
    "    preprocessed_pixels = preprocess_pixels(pixels)\n",
    "    \n",
    "    # Use the preprocessed pixels to make predictions\n",
    "    prediction = model.predict(preprocessed_pixels)\n",
    "    \n",
    "    class_labels = ['Anger', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sadness', 'Surprise']\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "    image_class = class_labels[predicted_class_index]\n",
    "    # Convert the pixel values to an image\n",
    "    image = preprocessed_pixels.reshape(48, 48)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return image_class\n",
    "    return image_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Happy\n"
     ]
    }
   ],
   "source": [
    "image_path = 'test.jpg'\n",
    "predicted_class = classify_image(image_path)\n",
    "print('Predicted Class:', predicted_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
